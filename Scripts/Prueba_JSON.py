from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
import os
import json
from dotenv import load_dotenv
import csv # Importamos el m√≥dulo csv necesario
from typing import Dict # Para tipado

# Importa tu clase de agente
from agentes.Agente_semantico import BiologySemanticAgent

# Cargar las variables de entorno (Aseg√∫rate que el .env est√© en la ruta correcta)
load_dotenv('Hunger-Wings\.env')

# --- RUTAS Y CONFIGURACI√ìN ---
CSV_PATH = r'Hunger-Wings\database\SB_publication_PMC.csv'

# --- 0. Cargar y Mapear Metadatos del CSV ---

def load_metadata_from_csv(csv_filepath: str) -> Dict[str, str]:
    """
    Carga los t√≠tulos y links del CSV, mapeando el t√≠tulo a su URL.
    
    Se limpia la clave del t√≠tulo eliminando cualquier ruta y la extensi√≥n.
    """
    metadata_map = {}
    try:
        # Se asume que el CSV usa UTF-8 y las columnas son 'title' y 'link'
        with open(csv_filepath, mode='r', encoding='utf-8') as csvfile:
            # Usamos DictReader para manejar las columnas por nombre
            reader = csv.DictReader(csvfile)
            for row in reader:
                # Obtenemos el t√≠tulo del CSV (podr√≠a contener una ruta si el CSV no es perfecto)
                title_key_full = row.get('title', '').strip()
                
                # 1. Eliminamos la ruta completa, dejando solo el nombre del archivo (ej: 'archivo.txt').
                filename_with_ext = os.path.basename(title_key_full)
                
                # 2. Eliminamos la extensi√≥n del archivo (ej: '.txt') para estandarizar la clave.
                title_key_clean, _ = os.path.splitext(filename_with_ext)
                
                link_value = row.get('link', '').strip()
                
                if title_key_clean and link_value:
                    # La clave del mapa es el t√≠tulo limpio (solo nombre del archivo, sin ruta ni extensi√≥n)
                    metadata_map[title_key_clean] = link_value
        print(f"DEBUG: CSV cargado exitosamente. Se mapearon {len(metadata_map)} registros de metadatos.")
        return metadata_map
    except FileNotFoundError:
        print(f"‚ö†Ô∏è Error: El archivo CSV no se encontr√≥ en la ruta: {csv_filepath}. Los links no estar√°n disponibles.")
        return {}
    except Exception as e:
        print(f"‚ùå Error al procesar el archivo CSV: {e}")
        return {}

ARTICLE_LINK_MAP = load_metadata_from_csv(CSV_PATH)


# ----------------------------------------------------
## 1. Configuraci√≥n del LLM
# ----------------------------------------------------

# VERIFICACI√ìN DE LA API KEY (Evita DefaultCredentialsError)
gemini_key = os.getenv('GEMINI_API_KEY')
if not gemini_key:
    raise ValueError(
        "ERROR: La variable GEMINI_API_KEY no se encontr√≥ despu√©s de cargar el .env. "
        "¬°Revisa tu archivo y nombre de variable!"
    )
print(f"DEBUG: GEMINI_API_KEY cargada correctamente (longitud: {len(gemini_key)} caracteres)")

# Inicializaci√≥n de Gemini LLM (response_mime_type es esencial para el JSON)
gemini_llm = ChatGoogleGenerativeAI(
    model='gemini-2.5-flash',
    api_key=gemini_key,
    response_mime_type="application/json"
)

# ----------------------------------------------------
## 2. Configuraci√≥n de RAG
# ----------------------------------------------------

# Configuraci√≥n de Embeddings (Debe coincidir con la ingesta)
embeddings_generator = OllamaEmbeddings(
    model='mxbai-embed-large'
)

# Configuraci√≥n de Chroma Vector Store (Aseg√∫rate de la ruta y collection_name)
vector_store = Chroma(
    collection_name='biology',
    embedding_function=embeddings_generator,
    persist_directory='Hunger-Wings\database\chromadb'
)

# Instanciaci√≥n del Agente Biol√≥gico
# PASAMOS EL MAPA DE LINKS AL AGENTE
agent = BiologySemanticAgent(
    vector_store,
    gemini_llm,
    article_link_map=ARTICLE_LINK_MAP # <--- NUEVO ARGUMENTO: El agente debe usar este mapa
)

# ----------------------------------------------------
## 3. Prueba de Recuperaci√≥n (Diagn√≥stico del Problema)
# ----------------------------------------------------
question = 'Explica los procedimientos experimentales con animales para la misi√≥n STS-131'
k_chunks = 5

print(f"\n--- üß™ Probando Recuperaci√≥n Cruda de Chroma para '{question[:50]}...' ---")

# Ejecuta la recuperaci√≥n directamente sobre la base de datos
test_results = vector_store.similarity_search(question, k_chunks)

if not test_results:
    print("\nüö® ERROR CR√çTICO DE RECUPERACI√ìN: Chroma no devolvi√≥ ning√∫n documento para la colecci√≥n 'biology'.")
    print("Posibles Causas:")
    print("1. La base de datos en './database/chromadb' est√° vac√≠a o no existe.")
    print("2. El nombre de la colecci√≥n ('biology') es incorrecto.")
    print("3. El modelo de embeddings ('mxbai-embed-large') es diferente al usado durante la ingesta.")
    exit()
else:
    print(f"\n‚úÖ √âXITO DE RECUPERACI√ìN: Chroma encontr√≥ {len(test_results)} fragmentos.")
    
    # --- MODIFICACI√ìN CLAVE AQU√ç ---
    # Procesamos los metadatos del primer fragmento
    first_chunk = test_results[0]
    source_full_path = first_chunk.metadata.get('source', 'N/A')
    
    # Aplicar la misma l√≥gica de limpieza que en load_metadata_from_csv
    if source_full_path != 'N/A':
        # 1. Obtener solo el nombre del archivo con extensi√≥n
        filename_with_ext = os.path.basename(source_full_path)
        # 2. Eliminar la extensi√≥n
        source_clean, _ = os.path.splitext(filename_with_ext)
        
        # 3. ACTUALIZAR la metadata para el JSON de salida con el t√≠tulo limpio
        # Esto asegura que el campo 'source' en la metadata del documento (que es el que se pasa al LLM)
        # contenga solo el nombre del archivo sin la extensi√≥n.
        first_chunk.metadata['source'] = source_clean
    else:
        source_clean = 'N/A'
    
    # Imprime una parte del contenido para confirmar que es relevante, usando el t√≠tulo limpio
    # ¬°OJO! Ahora 'source' ya contiene el t√≠tulo limpio gracias a la modificaci√≥n anterior.
    print(f"Fragmento 1 (Source: {first_chunk.metadata.get('source')}): {first_chunk.page_content[:150]}...")
    
print("--------------------------------------------------")

# ----------------------------------------------------
## 4. Ejecuci√≥n de la Generaci√≥n JSON
# ----------------------------------------------------

print(f"\n--- ü§ñ Generando Respuesta Estructurada con Gemini ---")

# Llamada al Agente para generar la respuesta (retorna un dict/JSON ya parseado o None)
# NOTA: Ahora el agente usar√° 'article_link_map' para encontrar el link de cada art√≠culo en el resultado.
json_response_dict = agent.generate(question, k_chunks)

# ----------------------------------------------------
## 5. Impresi√≥n del Resultado Final
# ----------------------------------------------------
if json_response_dict:
    print("\n‚úÖ Generaci√≥n de JSON Exitosa. Resultado para el Frontend:")
    print(json.dumps(json_response_dict, indent=2, ensure_ascii=False))
else:
    print("\n‚ùå FALLO EN LA GENERACI√ìN DE RESPUESTA. Esto SUCEDE porque el LLM devolvi√≥ un JSON inv√°lido.")